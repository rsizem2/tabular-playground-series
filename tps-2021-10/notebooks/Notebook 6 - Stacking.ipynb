{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d62c8754",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "In this notebook we look at the best parameters found for the following models:\n",
    "\n",
    "1. XGBoost\n",
    "2. LightGBM\n",
    "3. CatBoost\n",
    "4. HistGradientBoosting (scikit-learn)\n",
    "\n",
    "We then use stacking to ensemble these 4 models.\n",
    "\n",
    "**Note:** I leave the models on their verbose settings so I can monitor their training since it will take a long time to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83579a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for testing changes to this notebook quickly\n",
    "RANDOM_SEED = 0\n",
    "NUM_TREES = 15000\n",
    "EARLY_STOP = 200\n",
    "NUM_FOLDS = 3\n",
    "TEST = False\n",
    "SUBMIT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "404500ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import pyarrow\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Evaluation and model selection\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "\n",
    "# Models\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# Hide warnings (makes optuna output easier to parse)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420853e1",
   "metadata": {},
   "source": [
    "# Preparing the Data\n",
    "\n",
    "We define our cross-validation scheme at the start to ensure that it is the same across all the models we consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "634d9a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load Data\n",
    "train = pd.read_feather(\"../data/train.feather\")\n",
    "test = pd.read_feather(\"../data/test.feather\")\n",
    "submission = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "if TEST:\n",
    "    train, junk = train_test_split(\n",
    "        train, \n",
    "        train_size = 0.1,\n",
    "        shuffle = True,\n",
    "        stratify = train['target'],\n",
    "    )\n",
    "    train.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    del junk\n",
    "    gc.collect()\n",
    "\n",
    "# Relevant features\n",
    "features = [x for x in train.columns if x not in ['id','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a7d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified k-fold cross-validation\n",
    "train['kfold'] = -1\n",
    "skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['target'])):\n",
    "    train['kfold'].iloc[valid_idx] = fold\n",
    "    \n",
    "oof_preds = pd.DataFrame(\n",
    "    data = dict(kfold = train['kfold'])\n",
    ")\n",
    "\n",
    "test_preds = pd.DataFrame(\n",
    "    data = dict(id = test['id'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c5efb4",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "We experiment with feature engineering using row statistics, primarily to add variance to our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc73e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_row_stats(data):\n",
    "    cont_cols, cat_cols = list(), list()\n",
    "    for col in features:\n",
    "        if data[col].dtype.name.startswith(\"int\"):\n",
    "            cat_cols.append(col)\n",
    "        else:\n",
    "            cont_cols.append(col)\n",
    "    new_data = data.copy()\n",
    "    new_data['binary_count'] = data[cat_cols].sum(axis=1)\n",
    "    new_data['binary_std'] = data[cat_cols].std(axis=1)\n",
    "    new_data['min'] = data[cont_cols].min(axis=1)\n",
    "    new_data['std'] = data[cont_cols].std(axis=1)\n",
    "    new_data['max'] = data[cont_cols].max(axis=1)\n",
    "    new_data['median'] = data[cont_cols].median(axis=1)\n",
    "    new_data['mean'] = data[cont_cols].mean(axis=1)\n",
    "    #new_data['var'] = data[cont_cols].var(axis=1)\n",
    "    #new_data['sum'] = data[cont_cols].sum(axis=1)\n",
    "    #new_data['sem'] = data[cont_cols].sem(axis=1)\n",
    "    new_data['skew'] = data[cont_cols].skew(axis=1)\n",
    "    new_data['median_abs_dev'] = stats.median_abs_deviation(data[cont_cols], axis=1)\n",
    "    new_data['zscore'] = (np.abs(stats.zscore(data[cont_cols]))).sum(axis=1)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e1bcd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train = create_row_stats(train)\n",
    "test = create_row_stats(test)\n",
    "\n",
    "# New features\n",
    "all_features = [x for x in train.columns if x not in ['id','target','kfold']]\n",
    "assert features != all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad17bcf",
   "metadata": {},
   "source": [
    "# 1. XGBoost\n",
    "\n",
    "We use the best parameters from [this Kaggle notebook](https://www.kaggle.com/rsizem2/tps-10-21-optuna-w-pruning-callbacks-xgboost). Except for using CPU rather than GPU, which in a lot of cases results in more accurate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b182d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Parameters\n",
    "xgboost_params = {\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'n_estimators': NUM_TREES,\n",
    "    #'tree_method': 'hist',\n",
    "    'max_depth': 5, \n",
    "    'learning_rate': 0.02261104274598307, \n",
    "    'min_child_weight': 74.7573299373233, \n",
    "    'subsample': 0.766, \n",
    "    'colsample_bytree': 0.268, \n",
    "    'colsample_bylevel': 0.591, \n",
    "    'reg_lambda': 75.35694292360638\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0afe685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(model_params = {}, fit_params = {}, new_features = False):\n",
    "    \n",
    "    # Store the  predictions\n",
    "    oof_preds = np.zeros((train.shape[0],))\n",
    "    test_preds = np.zeros((test.shape[0],))\n",
    "    print('')\n",
    "    \n",
    "    # Stratified k-fold cross-validation\n",
    "    for fold in range(NUM_FOLDS):\n",
    "        \n",
    "        # Training and Validation Sets\n",
    "        if new_features:\n",
    "            X_train, y_train = train[train.kfold != fold][features], train[train.kfold != fold]['target']\n",
    "            X_valid, y_valid = train[train.kfold == fold][features], train[train.kfold == fold]['target']\n",
    "            X_test = test[features]\n",
    "        else:\n",
    "            X_train, y_train = train[train.kfold != fold][all_features], train[train.kfold != fold]['target']\n",
    "            X_valid, y_valid = train[train.kfold == fold][all_features], train[train.kfold == fold]['target']\n",
    "            X_test = test[all_features]\n",
    "        \n",
    "        # Define Model\n",
    "        model = XGBClassifier(**{**xgboost_params, **model_params})\n",
    "        gc.collect()\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            verbose = False,\n",
    "            eval_set = [(X_valid, y_valid)],\n",
    "            eval_metric = \"auc\",\n",
    "            early_stopping_rounds = EARLY_STOP,\n",
    "            **fit_params\n",
    "        )\n",
    "        \n",
    "        # validation and test predictions\n",
    "        valid_preds = model.predict_proba(X_valid)[:, 1]\n",
    "        test_preds += model.predict_proba(X_test)[:, 1] / NUM_FOLDS\n",
    "        oof_preds[train.kfold == fold] = valid_preds\n",
    "        \n",
    "        # fold auc score\n",
    "        fold_auc = roc_auc_score(y_valid, valid_preds)\n",
    "        end = time.time()\n",
    "        print(f'Fold {fold} (AUC): {round(fold_auc, 5)} in {round(end - start, 2)}s.')\n",
    "        \n",
    "    return test_preds, oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0047d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 (AUC): 0.85685 in 2180.56s.\n",
      "Fold 1 (AUC): 0.85702 in 2497.38s.\n",
      "Fold 2 (AUC): 0.85683 in 2284.06s.\n",
      "\n",
      "Fold 0 (AUC): 0.85688 in 1511.14s.\n",
      "Fold 1 (AUC): 0.85698 in 1625.21s.\n",
      "Fold 2 (AUC): 0.8568 in 1430.2s.\n",
      "\n",
      "Fold 0 (AUC): 0.85691 in 1962.9s.\n",
      "Fold 1 (AUC): 0.85697 in 2149.46s.\n",
      "Fold 2 (AUC): 0.85674 in 2147.12s.\n"
     ]
    }
   ],
   "source": [
    "# Train 3 models \n",
    "test_preds['XGBoost'], oof_preds['XGBoost'] = train_xgboost()\n",
    "test_preds['XGB_Hist'], oof_preds['XGB_Hist'] = train_xgboost(\n",
    "    model_params = dict(tree_method = 'hist')\n",
    ")\n",
    "test_preds['XGB_Stats'], oof_preds['XGB_Stats'] = train_xgboost(new_features = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b4e96d",
   "metadata": {},
   "source": [
    "# 2. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5669d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Parameters\n",
    "lightgbm_params = {\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'n_estimators': NUM_TREES,\n",
    "    'max_depth': 6, \n",
    "    'learning_rate': 0.009099999999999999, \n",
    "    'min_child_samples': 4260, \n",
    "    'subsample': 0.87, \n",
    "    'subsample_freq': 3, \n",
    "    'colsample_bytree': 0.27, \n",
    "    'reg_lambda': 0.0003694272556917343, \n",
    "    'num_leaves': 26,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fb39aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm(model_params = {}, fit_params = {}, new_features = False):\n",
    "    \n",
    "    # Store the holdout predictions\n",
    "    oof_preds = np.zeros((train.shape[0],))\n",
    "    test_preds = np.zeros((test.shape[0],))\n",
    "    print('')\n",
    "    \n",
    "    # Stratified k-fold cross-validation\n",
    "    for fold in range(NUM_FOLDS):\n",
    "        \n",
    "        # Training and Validation Sets\n",
    "        if new_features:\n",
    "            X_train, y_train = train[train.kfold != fold][features], train[train.kfold != fold]['target']\n",
    "            X_valid, y_valid = train[train.kfold == fold][features], train[train.kfold == fold]['target']\n",
    "            X_test = test[features]\n",
    "        else:\n",
    "            X_train, y_train = train[train.kfold != fold][all_features], train[train.kfold != fold]['target']\n",
    "            X_valid, y_valid = train[train.kfold == fold][all_features], train[train.kfold == fold]['target']\n",
    "            X_test = test[all_features]\n",
    "        \n",
    "        # Define Model\n",
    "        model = LGBMClassifier(**{**lightgbm_params, **model_params})\n",
    "        gc.collect()\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            verbose = 0,\n",
    "            eval_set = [(X_valid, y_valid)],\n",
    "            eval_metric = \"auc\",\n",
    "            early_stopping_rounds = EARLY_STOP,\n",
    "            **fit_params\n",
    "        )\n",
    "        \n",
    "        # validation and test predictions\n",
    "        valid_preds = model.predict_proba(X_valid)[:, 1]\n",
    "        test_preds += model.predict_proba(X_test)[:, 1] / NUM_FOLDS\n",
    "        oof_preds[train.kfold == fold] = valid_preds\n",
    "        \n",
    "        # fold auc score\n",
    "        fold_auc = roc_auc_score(y_valid, valid_preds)\n",
    "        end = time.time()\n",
    "        print(f'Fold {fold} (AUC): {round(fold_auc, 5)} in {round(end - start, 2)}s.')\n",
    "        \n",
    "    return test_preds, oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdf0373c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 (AUC): 0.85706 in 584.37s.\n",
      "Fold 1 (AUC): 0.8571 in 682.35s.\n",
      "Fold 2 (AUC): 0.85693 in 645.17s.\n",
      "\n",
      "Fold 0 (AUC): 0.85706 in 621.14s.\n",
      "Fold 1 (AUC): 0.85713 in 650.93s.\n",
      "Fold 2 (AUC): 0.85689 in 629.13s.\n"
     ]
    }
   ],
   "source": [
    "# Train 2 models\n",
    "test_preds['LightGBM'], oof_preds['LightGBM'] = train_lightgbm()\n",
    "test_preds['LGBM_Stats'], oof_preds['LGBM_Stats'] = train_lightgbm(new_features = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c5214a",
   "metadata": {},
   "source": [
    "# 3. CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d119bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Parameters\n",
    "catboost_params = {\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'n_estimators': NUM_TREES,\n",
    "    'boosting_type': 'Plain',\n",
    "    'bootstrap_type': 'Bernoulli',\n",
    "    'early_stopping_rounds': EARLY_STOP,\n",
    "    'eval_metric': 'AUC',\n",
    "    'max_depth': 7, \n",
    "    'learning_rate': 0.01, \n",
    "    'min_child_samples': 12710, \n",
    "    'random_strength': 33.21156029537479, \n",
    "    'leaf_estimation_iterations': 1, \n",
    "    'subsample': 0.6990000000000001, \n",
    "    'reg_lambda': 60.52806724303393\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b67def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_catboost(model_params = {}, fit_params = {}, new_features = False):\n",
    "    \n",
    "    # Store the predictions\n",
    "    oof_preds = np.zeros((train.shape[0],))\n",
    "    test_preds = np.zeros((test.shape[0],))\n",
    "    print('')\n",
    "\n",
    "    # Stratified k-fold cross-validation\n",
    "    for fold in range(NUM_FOLDS):\n",
    "        \n",
    "        # Training and Validation Sets\n",
    "        if new_features:\n",
    "            X_train, y_train = train[train.kfold != fold][features], train[train.kfold != fold]['target']\n",
    "            X_valid, y_valid = train[train.kfold == fold][features], train[train.kfold == fold]['target']\n",
    "            X_test = test[features]\n",
    "        else:\n",
    "            X_train, y_train = train[train.kfold != fold][all_features], train[train.kfold != fold]['target']\n",
    "            X_valid, y_valid = train[train.kfold == fold][all_features], train[train.kfold == fold]['target']\n",
    "            X_test = test[all_features]\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        # Define Model\n",
    "        model = CatBoostClassifier(**{**catboost_params, **model_params})\n",
    "        gc.collect()\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            verbose = False,\n",
    "            eval_set = [(X_valid, y_valid)],\n",
    "            use_best_model = True,\n",
    "            **fit_params\n",
    "        )\n",
    "        \n",
    "        # validation and test predictions\n",
    "        valid_preds = model.predict_proba(X_valid)[:, 1]\n",
    "        test_preds += model.predict_proba(X_test)[:, 1] / NUM_FOLDS\n",
    "        oof_preds[train.kfold == fold] = valid_preds\n",
    "        \n",
    "        # fold auc score\n",
    "        fold_auc = roc_auc_score(y_valid, valid_preds)\n",
    "        end = time.time()\n",
    "        print(f'Fold {fold} (AUC): {round(fold_auc, 5)} in {round(end - start, 2)}s.')\n",
    "        \n",
    "    return test_preds, oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22596ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 (AUC): 0.85667 in 2564.81s.\n",
      "Fold 1 (AUC): 0.85683 in 2898.82s.\n",
      "Fold 2 (AUC): 0.85663 in 2881.62s.\n",
      "\n",
      "Fold 0 (AUC): 0.85667 in 2802.61s.\n",
      "Fold 1 (AUC): 0.85675 in 2816.5s.\n",
      "Fold 2 (AUC): 0.85659 in 2820.11s.\n"
     ]
    }
   ],
   "source": [
    "# Train CatBoost\n",
    "test_preds['CatBoost'], oof_preds['CatBoost'] = train_catboost()\n",
    "test_preds['Cat_Stats'], oof_preds['Cat_Stats'] = train_catboost(new_features = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41292af8",
   "metadata": {},
   "source": [
    "# 4. Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcbd111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Parameters\n",
    "histgbc_params = {\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'max_iter': NUM_TREES,\n",
    "    'validation_fraction': 0.33,\n",
    "    'early_stopping': True,\n",
    "    'n_iter_no_change': EARLY_STOP,\n",
    "    'verbose': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02e1023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_histgbm(model_params = {}, fit_params = {}, new_features = False):\n",
    "    \n",
    "    # Store the predictions\n",
    "    oof_preds = np.zeros((train.shape[0],))\n",
    "    test_preds = np.zeros((test.shape[0],))\n",
    "    print('')\n",
    "    \n",
    "    # Stratified k-fold cross-validation\n",
    "    for fold in range(NUM_FOLDS):\n",
    "        \n",
    "        # Training and Validation Sets\n",
    "        if new_features:\n",
    "            X_train, y_train = train[train.kfold != fold][features], train[train.kfold != fold]['target']\n",
    "            X_valid, y_valid = train[train.kfold == fold][features], train[train.kfold == fold]['target']\n",
    "            X_test = test[features]\n",
    "        else:\n",
    "            X_train, y_train = train[train.kfold != fold][all_features], train[train.kfold != fold]['target']\n",
    "            X_valid, y_valid = train[train.kfold == fold][all_features], train[train.kfold == fold]['target']\n",
    "            X_test = test[all_features]\n",
    "        \n",
    "        # Define Model\n",
    "        model = HistGradientBoostingClassifier(**{**histgbc_params, **model_params})\n",
    "        gc.collect()\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            **fit_params\n",
    "        )\n",
    "        \n",
    "        # validation and test predictions\n",
    "        valid_preds = model.predict_proba(X_valid)[:, 1]\n",
    "        test_preds += model.predict_proba(X_test)[:, 1] / NUM_FOLDS\n",
    "        oof_preds[train.kfold == fold] = valid_preds\n",
    "        \n",
    "        # fold auc score\n",
    "        fold_auc = roc_auc_score(y_valid, valid_preds)\n",
    "        end = time.time()\n",
    "        print(f'Fold {fold} (AUC): {round(fold_auc, 5)} in {round(end - start, 2)}s.')\n",
    "        \n",
    "    return test_preds, oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4af55300",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 (AUC): 0.8542 in 156.65s.\n",
      "Fold 1 (AUC): 0.85421 in 159.53s.\n",
      "Fold 2 (AUC): 0.85438 in 161.03s.\n",
      "\n",
      "Fold 0 (AUC): 0.85407 in 173.18s.\n",
      "Fold 1 (AUC): 0.85388 in 159.88s.\n",
      "Fold 2 (AUC): 0.85394 in 162.2s.\n"
     ]
    }
   ],
   "source": [
    "# Train 2 models with different random seets\n",
    "test_preds['HistGBM'], oof_preds['HistGBM'] = train_histgbm()\n",
    "test_preds['Hist_Stats'], oof_preds['Hist_Stats'] = train_histgbm(new_features = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498766cb",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f9ad82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kfold</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>XGB_Hist</th>\n",
       "      <th>XGB_Stats</th>\n",
       "      <th>LightGBM</th>\n",
       "      <th>LGBM_Stats</th>\n",
       "      <th>CatBoost</th>\n",
       "      <th>Cat_Stats</th>\n",
       "      <th>HistGBM</th>\n",
       "      <th>Hist_Stats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.695719</td>\n",
       "      <td>0.674580</td>\n",
       "      <td>0.663080</td>\n",
       "      <td>0.677566</td>\n",
       "      <td>0.688227</td>\n",
       "      <td>0.642127</td>\n",
       "      <td>0.647414</td>\n",
       "      <td>0.695878</td>\n",
       "      <td>0.633604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.197818</td>\n",
       "      <td>0.209527</td>\n",
       "      <td>0.198422</td>\n",
       "      <td>0.195066</td>\n",
       "      <td>0.185122</td>\n",
       "      <td>0.228275</td>\n",
       "      <td>0.224138</td>\n",
       "      <td>0.156371</td>\n",
       "      <td>0.246440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.865054</td>\n",
       "      <td>0.844144</td>\n",
       "      <td>0.858808</td>\n",
       "      <td>0.860939</td>\n",
       "      <td>0.850658</td>\n",
       "      <td>0.866861</td>\n",
       "      <td>0.858791</td>\n",
       "      <td>0.861330</td>\n",
       "      <td>0.851976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.465767</td>\n",
       "      <td>0.487962</td>\n",
       "      <td>0.477282</td>\n",
       "      <td>0.474840</td>\n",
       "      <td>0.492786</td>\n",
       "      <td>0.495233</td>\n",
       "      <td>0.499936</td>\n",
       "      <td>0.320400</td>\n",
       "      <td>0.526619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.846476</td>\n",
       "      <td>0.846924</td>\n",
       "      <td>0.867860</td>\n",
       "      <td>0.861961</td>\n",
       "      <td>0.857914</td>\n",
       "      <td>0.847776</td>\n",
       "      <td>0.851610</td>\n",
       "      <td>0.819631</td>\n",
       "      <td>0.845977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kfold   XGBoost  XGB_Hist  XGB_Stats  LightGBM  LGBM_Stats  CatBoost  \\\n",
       "0      1  0.695719  0.674580   0.663080  0.677566    0.688227  0.642127   \n",
       "1      1  0.197818  0.209527   0.198422  0.195066    0.185122  0.228275   \n",
       "2      2  0.865054  0.844144   0.858808  0.860939    0.850658  0.866861   \n",
       "3      0  0.465767  0.487962   0.477282  0.474840    0.492786  0.495233   \n",
       "4      0  0.846476  0.846924   0.867860  0.861961    0.857914  0.847776   \n",
       "\n",
       "   Cat_Stats   HistGBM  Hist_Stats  \n",
       "0   0.647414  0.695878    0.633604  \n",
       "1   0.224138  0.156371    0.246440  \n",
       "2   0.858791  0.861330    0.851976  \n",
       "3   0.499936  0.320400    0.526619  \n",
       "4   0.851610  0.819631    0.845977  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba56a63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>XGB_Hist</th>\n",
       "      <th>XGB_Stats</th>\n",
       "      <th>LightGBM</th>\n",
       "      <th>LGBM_Stats</th>\n",
       "      <th>CatBoost</th>\n",
       "      <th>Cat_Stats</th>\n",
       "      <th>HistGBM</th>\n",
       "      <th>Hist_Stats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000</td>\n",
       "      <td>0.742483</td>\n",
       "      <td>0.738944</td>\n",
       "      <td>0.743030</td>\n",
       "      <td>0.740738</td>\n",
       "      <td>0.733244</td>\n",
       "      <td>0.732316</td>\n",
       "      <td>0.737226</td>\n",
       "      <td>0.691237</td>\n",
       "      <td>0.650669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000001</td>\n",
       "      <td>0.245132</td>\n",
       "      <td>0.250275</td>\n",
       "      <td>0.240614</td>\n",
       "      <td>0.251458</td>\n",
       "      <td>0.245210</td>\n",
       "      <td>0.262507</td>\n",
       "      <td>0.236537</td>\n",
       "      <td>0.267694</td>\n",
       "      <td>0.264484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000002</td>\n",
       "      <td>0.906799</td>\n",
       "      <td>0.908351</td>\n",
       "      <td>0.908237</td>\n",
       "      <td>0.907186</td>\n",
       "      <td>0.906372</td>\n",
       "      <td>0.905988</td>\n",
       "      <td>0.910130</td>\n",
       "      <td>0.877557</td>\n",
       "      <td>0.891221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000003</td>\n",
       "      <td>0.818379</td>\n",
       "      <td>0.820664</td>\n",
       "      <td>0.851904</td>\n",
       "      <td>0.834636</td>\n",
       "      <td>0.850805</td>\n",
       "      <td>0.842568</td>\n",
       "      <td>0.854714</td>\n",
       "      <td>0.841316</td>\n",
       "      <td>0.832215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000004</td>\n",
       "      <td>0.255338</td>\n",
       "      <td>0.251446</td>\n",
       "      <td>0.258755</td>\n",
       "      <td>0.265646</td>\n",
       "      <td>0.263108</td>\n",
       "      <td>0.279009</td>\n",
       "      <td>0.274908</td>\n",
       "      <td>0.243637</td>\n",
       "      <td>0.254913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   XGBoost  XGB_Hist  XGB_Stats  LightGBM  LGBM_Stats  CatBoost  \\\n",
       "0  1000000  0.742483  0.738944   0.743030  0.740738    0.733244  0.732316   \n",
       "1  1000001  0.245132  0.250275   0.240614  0.251458    0.245210  0.262507   \n",
       "2  1000002  0.906799  0.908351   0.908237  0.907186    0.906372  0.905988   \n",
       "3  1000003  0.818379  0.820664   0.851904  0.834636    0.850805  0.842568   \n",
       "4  1000004  0.255338  0.251446   0.258755  0.265646    0.263108  0.279009   \n",
       "\n",
       "   Cat_Stats   HistGBM  Hist_Stats  \n",
       "0   0.737226  0.691237    0.650669  \n",
       "1   0.236537  0.267694    0.264484  \n",
       "2   0.910130  0.877557    0.891221  \n",
       "3   0.854714  0.841316    0.832215  \n",
       "4   0.274908  0.243637    0.254913  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915edb2f",
   "metadata": {},
   "source": [
    "# Generate Submissions\n",
    "\n",
    "We create submissions for the CPU generated predictions to see if they are better than the GPU generated models we created with Kaggle notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58ca0951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make submission\n",
    "submission['target'] = test_preds['XGBoost']\n",
    "if SUBMIT: submission.to_csv(f'../output/xgboost_cpu_{NUM_FOLDS}fold_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57832f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make submission\n",
    "submission['target'] = test_preds['CatBoost']\n",
    "if SUBMIT: submission.to_csv(f'../output/catboost_cpu_{NUM_FOLDS}fold_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea8a1a",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "We use XGBoost and LightGBM as meta models for stacking:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e37044",
   "metadata": {},
   "source": [
    "## 1. LightGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb6129ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_lightgbm():\n",
    "    preds = np.zeros((test.shape[0],))\n",
    "    scores = np.zeros(NUM_FOLDS)\n",
    "    \n",
    "    for j in range(NUM_FOLDS):\n",
    "        X_train = oof_preds[oof_preds.kfold != j].drop('kfold', axis = 1)\n",
    "        X_valid = oof_preds[oof_preds.kfold == j].drop('kfold', axis = 1)\n",
    "        y_train = train['target'][train.kfold != j]\n",
    "        y_valid = train['target'][train.kfold == j]\n",
    "        X_test = test_preds.drop('id', axis = 1)\n",
    "\n",
    "        model = LGBMClassifier(random_state = RANDOM_SEED, n_estimators = 200)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            verbose = 0,\n",
    "            eval_set = [(X_valid, y_valid)],\n",
    "            eval_metric = \"auc\",\n",
    "            early_stopping_rounds = 25,\n",
    "        )\n",
    "\n",
    "        preds += model.predict_proba(X_test)[:, 1] / NUM_FOLDS \n",
    "        preds_valid = model.predict_proba(X_valid)[:, 1]\n",
    "        scores[j] = roc_auc_score(y_valid, preds_valid)\n",
    "        print(\"Fold\", j ,\"(AUC):\", scores[j])\n",
    "\n",
    "    print(\"Avg (AUC):\", round(scores.mean(),6))\n",
    "    print(\"Min (AUC):\", round(scores.min(),6))\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a1831c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 (AUC): 0.8571198053744282\n",
      "Fold 1 (AUC): 0.8572540298873037\n",
      "Fold 2 (AUC): 0.8570818167391762\n",
      "Avg (AUC): 0.857152\n",
      "Min (AUC): 0.857082\n"
     ]
    }
   ],
   "source": [
    "# LGBMClassifier meta model\n",
    "submission['target'] = stack_lightgbm()\n",
    "if SUBMIT: submission.to_csv(f'../output/stack_lgbm_{NUM_FOLDS}fold_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00362f4a",
   "metadata": {},
   "source": [
    "## 2. XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11cc8235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_xgboost():\n",
    "    preds = np.zeros((test.shape[0],))\n",
    "    scores = np.zeros(NUM_FOLDS)\n",
    "    \n",
    "    for j in range(NUM_FOLDS):\n",
    "        X_train = oof_preds[oof_preds.kfold != j].drop('kfold', axis = 1)\n",
    "        X_valid = oof_preds[oof_preds.kfold == j].drop('kfold', axis = 1)\n",
    "        y_train = train['target'][train.kfold != j]\n",
    "        y_valid = train['target'][train.kfold == j]\n",
    "        X_test = test_preds.drop('id', axis = 1)\n",
    "\n",
    "        model = XGBClassifier(random_state = RANDOM_SEED, n_estimators = 200)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            verbose = False,\n",
    "            eval_set = [(X_valid, y_valid)],\n",
    "            eval_metric = \"auc\",\n",
    "            early_stopping_rounds = 25,\n",
    "        )\n",
    "\n",
    "        preds += model.predict_proba(X_test)[:, 1] / NUM_FOLDS \n",
    "        preds_valid = model.predict_proba(X_valid)[:, 1]\n",
    "        scores[j] = roc_auc_score(y_valid, preds_valid)\n",
    "        print(\"Fold\", j ,\"(AUC):\", scores[j])\n",
    "\n",
    "    print(\"Avg (AUC):\", round(scores.mean(),6))\n",
    "    print(\"Min (AUC):\", round(scores.min(),6))\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dfbfda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 (AUC): 0.8570661991082071\n",
      "Fold 1 (AUC): 0.8571657333818037\n",
      "Fold 2 (AUC): 0.8570078618118256\n",
      "Avg (AUC): 0.85708\n",
      "Min (AUC): 0.857008\n"
     ]
    }
   ],
   "source": [
    "# XGBClassifier meta model\n",
    "submission['target'] = stack_xgboost()\n",
    "if SUBMIT: submission.to_csv(f'../output/stack_xgb_{NUM_FOLDS}fold_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
