{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a999e9",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "In this notebook we test out several feature engineering techniques. In particular, we will try out the following features:\n",
    "\n",
    "1. Feature Selection\n",
    "2. Row statistics (static features)\n",
    "3. TargetEncoding\n",
    "4. KMeans Clustering\n",
    "\n",
    "In each case we will compare it with the baseline LightGBM model and score it using cross-validation. For each technique we use the following parameters:\n",
    "\n",
    "* `n_estimators = 10000` with `early_stopping_rounds = 150`\n",
    "* `learning_rate = 0.03`\n",
    "* `random_state = 0` to ensure reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b69b0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for testing changes to this notebook quickly\n",
    "NUM_TREES = 10000\n",
    "EARLY_STOP = 150\n",
    "NUM_FOLDS = 3\n",
    "RANDOM_SEED = 0\n",
    "SUBMIT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6723da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import pyarrow\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# feature engineering\n",
    "import scipy.stats as stats\n",
    "from category_encoders import MEstimateEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from functools import partial\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# LightGBM\n",
    "from lightgbm import LGBMClassifier, plot_importance\n",
    "\n",
    "# Mute warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# display options\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6148a0ae",
   "metadata": {},
   "source": [
    "## Loading Function\n",
    "\n",
    "We create a function that recreates the training and holdout sets since some of our methods may overwrite the original data and we need a reproducible way to get the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b527c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training and holdout set\n",
    "def get_training_data():\n",
    "    train = pd.read_feather(\"../data/train.feather\")\n",
    "\n",
    "    train, holdout = train_test_split(\n",
    "        train,\n",
    "        train_size = 500000,\n",
    "        stratify = train['target'],\n",
    "        shuffle = True,\n",
    "        random_state = RANDOM_SEED,\n",
    "    )\n",
    "\n",
    "    train.reset_index(drop = True, inplace = True)\n",
    "    holdout.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    return train, holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a44694d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train, holdout = get_training_data()\n",
    "\n",
    "# save important features\n",
    "features = [x for x in train.columns if x not in ['id','target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8fec5",
   "metadata": {},
   "source": [
    "## Scoring Function\n",
    "\n",
    "For each feature engineering technique we create a function that accepts the training, test and validation data as arguments and returns the appropriately transformed data (taking care to avoid leakage). This function is passed to a scoring function as the argument `preprocessing`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27e81869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_lightgbm(preprocessing = None):\n",
    "    start = time.time()\n",
    "    holdout_preds = np.zeros((holdout.shape[0],))\n",
    "    print('')\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = 0)\n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['target'])):\n",
    "        \n",
    "        # train, valid split for cross-validation\n",
    "        X_train, y_train = train[features].iloc[train_idx].copy(), train['target'].iloc[train_idx].copy()\n",
    "        X_valid, y_valid = train[features].iloc[valid_idx].copy(), train['target'].iloc[valid_idx].copy()\n",
    "        X_test, y_test = holdout[features].copy(), holdout['target'].copy()\n",
    "        \n",
    "        # preprocessing function should return a copy\n",
    "        if preprocessing:\n",
    "            try:\n",
    "                X_train, X_valid, X_test = preprocessing(X_train, X_valid, X_test, y_train)\n",
    "            except:\n",
    "                X_train, X_valid, X_test = preprocessing(X_train, X_valid, X_test)\n",
    "        \n",
    "        # model with params\n",
    "        model = LGBMClassifier(\n",
    "            n_estimators = NUM_TREES,\n",
    "            random_state = RANDOM_SEED,\n",
    "            learning_rate = 0.03,\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set = [(X_valid, y_valid)],\n",
    "            eval_metric = 'auc',\n",
    "            early_stopping_rounds = EARLY_STOP,\n",
    "            verbose = False,\n",
    "        )\n",
    "\n",
    "        holdout_preds += model.predict_proba(X_test)[:,1] / NUM_FOLDS\n",
    "        valid_preds = model.predict_proba(X_valid)[:,1]\n",
    "        \n",
    "        fold_auc = roc_auc_score(y_valid, valid_preds)\n",
    "        print(f\"Fold {fold} (AUC):\", fold_auc)\n",
    "        \n",
    "    end = time.time()\n",
    "    return roc_auc_score(holdout['target'], holdout_preds), round(end-start, 2), model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9946be",
   "metadata": {},
   "source": [
    "# 0. Baseline (LightGBM)\n",
    "\n",
    "We start with computing a baseline score for LightGBM using the raw data with no feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec5032a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 (AUC): 0.8546680181750979\n",
      "Fold 1 (AUC): 0.8550275353925132\n",
      "Fold 2 (AUC): 0.8535062714075843\n",
      "\n",
      "Training Time: 565.08\n",
      "Holdout (AUC): 0.8562177395328208\n"
     ]
    }
   ],
   "source": [
    "baseline_score, baseline_time, model = score_lightgbm()\n",
    "\n",
    "print(\"\\nTraining Time:\", baseline_time)\n",
    "print(\"Holdout (AUC):\", baseline_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c350b04",
   "metadata": {},
   "source": [
    "# 1. Feature Selection\n",
    "\n",
    "In this section we experiment with dropping certain features deemed unimportant by various feature selection techniques. We consider two methods for determining unimportant features:\n",
    "\n",
    "* LightGBM feature importance\n",
    "* Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cc1f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structure for comparing\n",
    "data = dict(\n",
    "    scores = [baseline_score],\n",
    "    times = [baseline_time]\n",
    ")\n",
    "index = [\"Baseline\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0871ba5e",
   "metadata": {},
   "source": [
    "## 1.1 Feature Importance\n",
    "\n",
    "We define a bad feature as one with a feature importance below 3 using the building `feature_importance_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ded74f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine good columns\n",
    "good_columns = list()\n",
    "for score, col in zip(model.feature_importances_, train[features].columns):\n",
    "    if score >= 3:\n",
    "        good_columns.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4c9bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_importance(X_train, X_valid, X_test):\n",
    "    return X_train[good_columns], X_valid[good_columns], X_test[good_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3743bcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 14 features.\n",
      "\n",
      "Fold 0 (AUC): 0.8545783781620102\n",
      "Fold 1 (AUC): 0.8549845451883575\n",
      "Fold 2 (AUC): 0.853505431880075\n",
      "\n",
      "Training Time: 533.42\n",
      "Holdout (AUC): 0.856134647566317\n"
     ]
    }
   ],
   "source": [
    "# Feature selection with 'feature importance'\n",
    "print(f'Removed {len(features) - len(good_columns)} features.')\n",
    "fi_score, fi_time, model = score_lightgbm(feature_selection_importance)\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nTraining Time:\", fi_time)\n",
    "print(\"Holdout (AUC):\", fi_score)\n",
    "\n",
    "data['times'].append(fi_time)\n",
    "data['scores'].append(fi_score)\n",
    "index.append('Feature Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3902f6b3",
   "metadata": {},
   "source": [
    "## 1.2 Mutual Information\n",
    "\n",
    "In this section we remove features which have zero [mutual information](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif) scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba258a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_uninformative(X_train, X_valid, X_test, y_train, verbose = False):\n",
    "    \n",
    "    # 0. categoricals\n",
    "    binary_features = [X_train[x].dtype.name.startswith(\"int\") for x in X_train.columns]\n",
    "    \n",
    "    # 1. Determine uninformative columns\n",
    "    scores =  mutual_info_classif(\n",
    "        X_train, y_train,\n",
    "        discrete_features = binary_features,\n",
    "    )\n",
    "    cols = [x for i, x in enumerate(X_train.columns) if scores[i] == 0]\n",
    "    \n",
    "    # 2. Drop the uninformative columns\n",
    "    X_train.drop(cols, axis = 1, inplace = True)\n",
    "    X_valid.drop(cols, axis = 1, inplace = True)\n",
    "    X_test.drop(cols, axis = 1, inplace = True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Dropped columns:\", *cols)\n",
    "    \n",
    "    return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0892b872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 (AUC): 0.8479612625036981\n",
      "Fold 1 (AUC): 0.849321014621545\n",
      "Fold 2 (AUC): 0.8467167107565394\n",
      "\n",
      "Training Time: 1664.57\n",
      "Holdout (AUC): 0.8515919097064109\n"
     ]
    }
   ],
   "source": [
    "mi_score, mi_time, model = score_lightgbm(remove_uninformative)\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nTraining Time:\", mi_time)\n",
    "print(\"Holdout (AUC):\", mi_score)\n",
    "\n",
    "data['times'].append(mi_time)\n",
    "data['scores'].append(mi_score)\n",
    "index.append('Mutual Information')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81972b52",
   "metadata": {},
   "source": [
    "# 1. Row Statistics\n",
    "\n",
    "In this section, we calculate several row statistics as features and see which (if any) result in improvements over the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d890a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_row_stats(data):\n",
    "    cont_cols, cat_cols = list(), list()\n",
    "    for col in data.columns:\n",
    "        if data[col].dtype.name.startswith(\"int\"):\n",
    "            cat_cols.append(col)\n",
    "        else:\n",
    "            cont_cols.append(col)\n",
    "    new_data = data.copy()\n",
    "    new_data['binary_count'] = data[cat_cols].sum(axis=1)\n",
    "    new_data['binary_std'] = data[cat_cols].std(axis=1)\n",
    "    new_data['min'] = data[cont_cols].min(axis=1)\n",
    "    new_data['std'] = data[cont_cols].std(axis=1)\n",
    "    new_data['max'] = data[cont_cols].max(axis=1)\n",
    "    new_data['median'] = data[cont_cols].median(axis=1)\n",
    "    new_data['mean'] = data[cont_cols].mean(axis=1)\n",
    "    #new_data['var'] = data[cont_cols].var(axis=1)\n",
    "    #new_data['sum'] = data[cont_cols].sum(axis=1)\n",
    "    #new_data['sem'] = data[cont_cols].sem(axis=1)\n",
    "    new_data['skew'] = data[cont_cols].skew(axis=1)\n",
    "    new_data['median_abs_dev'] = stats.median_abs_deviation(data[cont_cols], axis=1)\n",
    "    new_data['zscore'] = (np.abs(stats.zscore(data[cont_cols]))).sum(axis=1)\n",
    "    return new_data\n",
    "\n",
    "def row_stats(X_train, X_valid, X_test, y_train):\n",
    "    X_train = create_row_stats(X_train)\n",
    "    X_valid = create_row_stats(X_valid)\n",
    "    X_test = create_row_stats(X_test)\n",
    "    return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71d12c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in train.columns if x not in ['id','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee17e976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 (AUC): 0.8546835329755162\n",
      "Fold 1 (AUC): 0.8549890739744684\n",
      "Fold 2 (AUC): 0.8535805096876286\n",
      "\n",
      "Training Time: 654.35\n",
      "Holdout (AUC): 0.8562796163752812\n"
     ]
    }
   ],
   "source": [
    "stats_score, stats_time, model = score_lightgbm(row_stats)\n",
    "\n",
    "print(\"\\nTraining Time:\", stats_time)\n",
    "print(\"Holdout (AUC):\", stats_score)\n",
    "\n",
    "data['times'].append(stats_time)\n",
    "data['scores'].append(stats_score)\n",
    "index.append('Row Stats')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f03408",
   "metadata": {},
   "source": [
    "We see that our model found some of these variables decently important for training however there is no noticable benefit to the overall model accuracy and a much slower training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bbe98d",
   "metadata": {},
   "source": [
    "# 2. Target Encoding\n",
    "\n",
    "In this section, we target encode all the binary variables. Target encoding is generally used for higher cardinality categorical data but we'll try it here anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f52e6bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train, holdout = get_training_data()\n",
    "\n",
    "features = [x for x in train.columns if x not in ['id','target']]\n",
    "binary_features = [x for x in features if train[x].dtype.name.startswith(\"int\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb8a98a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode(X_train, X_valid, X_test, y_train):\n",
    "    encoder = MEstimateEncoder(\n",
    "        cols = binary_features,\n",
    "        m = 1.0,\n",
    "    )\n",
    "    X_train = encoder.fit_transform(X_train, y_train)\n",
    "    X_valid = encoder.transform(X_valid)\n",
    "    X_test = encoder.transform(X_test)\n",
    "    return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75475afb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 (AUC): 0.8546217074851257\n",
      "Fold 1 (AUC): 0.8549852078743252\n",
      "Fold 2 (AUC): 0.8535062714075843\n",
      "\n",
      "Training Time: 698.17\n",
      "Holdout (AUC): 0.856247962553375\n"
     ]
    }
   ],
   "source": [
    "target_score, target_time, model = score_lightgbm(target_encode)\n",
    "\n",
    "# don't need the model\n",
    "del model\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nTraining Time:\", target_time)\n",
    "print(\"Holdout (AUC):\", target_score)\n",
    "\n",
    "data['times'].append(target_time)\n",
    "data['scores'].append(target_score)\n",
    "index.append('Target Encoding')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fbc0ce",
   "metadata": {},
   "source": [
    "As said before target encoding is best done with high cardinality variables so it's not particularly surprising that this didn't improve our models. It also significantly slowed down training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ed1da",
   "metadata": {},
   "source": [
    "# 3. KMeans Clustering\n",
    "\n",
    "We test cluster labels as categorical features and cluster distances as numerical features separately and see if either results in better models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b02a5d",
   "metadata": {},
   "source": [
    "## 3.1 Cluster Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "def9615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cluster_labels(X_train, X_valid, X_test, name, features, scale = True):\n",
    "    \n",
    "    # 1. normalize based on training data\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_train[features])\n",
    "        X_valid_scaled = scaler.transform(X_valid[features])\n",
    "        X_test_scaled = scaler.transform(X_test[features])\n",
    "    else:\n",
    "        # no scaling\n",
    "        X_scaled = X_train[features]\n",
    "        X_valid_scaled = X_valid[features]\n",
    "        X_test_scaled = X_test[features]\n",
    "    \n",
    "    # 2. create cluster labels (use predict)\n",
    "    kmeans = KMeans(\n",
    "        n_clusters = 10, \n",
    "        n_init = 10, \n",
    "        random_state = RANDOM_SEED\n",
    "    )\n",
    "    X_train[name + \"_Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "    X_valid[name + \"_Cluster\"] = kmeans.predict(X_valid_scaled)\n",
    "    X_test[name + \"_Cluster\"] = kmeans.predict(X_test_scaled)\n",
    "         \n",
    "    return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbf2c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_label_features(X_train, X_valid, X_test, y_train):\n",
    "    # get variables correlated with target\n",
    "    corr = train.corr()\n",
    "    corr = corr.loc['target':'target']\n",
    "    corr = corr.drop(['id','target'],axis=1)\n",
    "    corr = abs(corr)\n",
    "    corr = corr.sort_values(by='target',axis=1, ascending=False)\n",
    "    cols = [x for x in corr.columns][:15]\n",
    "    return generate_cluster_labels(X_train, X_valid, X_test, \"Top15\", cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6271bec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 (AUC): 0.854570248666942\n",
      "Fold 1 (AUC): 0.8548990131946711\n",
      "Fold 2 (AUC): 0.8534960185158745\n",
      "\n",
      "Training Time: 780.64\n",
      "Holdout (AUC): 0.856102446735894\n"
     ]
    }
   ],
   "source": [
    "clusterlabel_score, clusterlabel_time, model = score_lightgbm(cluster_label_features)\n",
    "\n",
    "# don't need the model\n",
    "del model\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nTraining Time:\", clusterlabel_time)\n",
    "print(\"Holdout (AUC):\", clusterlabel_score)\n",
    "\n",
    "data['times'].append(clusterlabel_time)\n",
    "data['scores'].append(clusterlabel_score)\n",
    "index.append(\"Cluster Labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df890d2",
   "metadata": {},
   "source": [
    "## 3.2 Cluster Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "424c361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cluster_distances(X_train, X_valid, X_test, name, features, scale = True):\n",
    "    \n",
    "    # 1. normalize based on training data\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_train[features])\n",
    "        X_valid_scaled = scaler.transform(X_valid[features])\n",
    "        X_test_scaled = scaler.transform(X_test[features])\n",
    "    else:\n",
    "        # no scaling\n",
    "        X_scaled = X_train[features]\n",
    "        X_valid_scaled = X_valid[features]\n",
    "        X_test_scaled = X_test[features]\n",
    "    \n",
    "    # 2. generate cluster distances (use transform)\n",
    "    kmeans = KMeans(n_clusters = 10, n_init = 10, random_state=0)\n",
    "    X_cd = kmeans.fit_transform(X_scaled)\n",
    "    X_valid_cd = kmeans.transform(X_valid_scaled)\n",
    "    X_test_cd = kmeans.transform(X_test_scaled)\n",
    "    \n",
    "    # 3. column labels\n",
    "    X_cd = pd.DataFrame(X_cd, columns=[name + \"_Centroid_\" + str(i) for i in range(X_cd.shape[1])])\n",
    "    X_valid_cd = pd.DataFrame(X_valid_cd, columns=[name + \"_Centroid_\" + str(i) for i in range(X_valid_cd.shape[1])])\n",
    "    X_test_cd = pd.DataFrame(X_test_cd, columns=[name + \"_Centroid_\" + str(i) for i in range(X_test_cd.shape[1])])    \n",
    "    \n",
    "    return X_train.join(X_cd), X_valid.join(X_valid_cd), X_test.join(X_test_cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea8bf716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_distance_features(X_train, X_valid, X_test, y_train):\n",
    "    # get variables correlated with target\n",
    "    corr = train.corr()\n",
    "    corr = corr.loc['target':'target']\n",
    "    corr = corr.drop(['id','target'],axis=1)\n",
    "    corr = abs(corr)\n",
    "    corr = corr.sort_values(by='target',axis=1, ascending=False)\n",
    "    cols = [x for x in corr.columns][:15]\n",
    "    return generate_cluster_distances(X_train, X_valid, X_test, \"Top15\", cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f7026cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 (AUC): 0.8545575676338328\n",
      "Fold 1 (AUC): 0.854876412463984\n",
      "Fold 2 (AUC): 0.8534671379055443\n",
      "\n",
      "Training Time: 860.15\n",
      "Holdout (AUC): 0.8561483535152661\n"
     ]
    }
   ],
   "source": [
    "clusterdist_score, clusterdist_time, model = score_lightgbm(cluster_distance_features)\n",
    "\n",
    "# don't need the model\n",
    "del model\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nTraining Time:\", clusterdist_time)\n",
    "print(\"Holdout (AUC):\", clusterdist_score)\n",
    "\n",
    "data['times'].append(clusterdist_time)\n",
    "data['scores'].append(clusterdist_score)\n",
    "index.append('Cluster Distances')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2033e2",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57fa48c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline</th>\n",
       "      <th>Feature Importance</th>\n",
       "      <th>Mutual Information</th>\n",
       "      <th>Row Stats</th>\n",
       "      <th>Target Encoding</th>\n",
       "      <th>Cluster Labels</th>\n",
       "      <th>Cluster Distances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scores</th>\n",
       "      <td>0.856218</td>\n",
       "      <td>0.856135</td>\n",
       "      <td>0.851592</td>\n",
       "      <td>0.85628</td>\n",
       "      <td>0.856248</td>\n",
       "      <td>0.856102</td>\n",
       "      <td>0.856148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>times</th>\n",
       "      <td>565.080000</td>\n",
       "      <td>533.420000</td>\n",
       "      <td>1664.570000</td>\n",
       "      <td>654.35000</td>\n",
       "      <td>698.170000</td>\n",
       "      <td>780.640000</td>\n",
       "      <td>860.150000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Baseline  Feature Importance  Mutual Information  Row Stats  \\\n",
       "scores    0.856218            0.856135            0.851592    0.85628   \n",
       "times   565.080000          533.420000         1664.570000  654.35000   \n",
       "\n",
       "        Target Encoding  Cluster Labels  Cluster Distances  \n",
       "scores         0.856248        0.856102           0.856148  \n",
       "times        698.170000      780.640000         860.150000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data = data, index = index).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f306fde",
   "metadata": {},
   "source": [
    "None of these methods appear particularly promising as they either provide no/little gain and/or increase the training time significantly but we may experiment with using some of these methods for ensembling to increase the variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
