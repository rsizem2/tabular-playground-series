{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68acfa96",
   "metadata": {},
   "source": [
    "# Baselines\n",
    "\n",
    "In this notebook we get baseline AUC scores for LightGBM's `LGBMClassifier` and scikit-learn's `HistGradientBoostingClassifier`. We use Kaggle notebooks to test out XGBoost and CatBoost since they both run very slowly on my local computer which has only CPU capabilities.\n",
    "\n",
    "In each case we use 3-fold cross-validation, fix the random seed, set a high value for the number of trees/iterations and use early stopping to avoid overfitting. Otherwise, we leave all settings at their defaults, the next few notebooks will be concerned with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f31fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for testing changes to this notebook quickly\n",
    "NUM_TREES = 10000\n",
    "EARLY_STOP = 150\n",
    "NUM_FOLDS = 3\n",
    "RANDOM_SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae43a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Models\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8c79db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training set\n",
    "train = pd.read_feather(\"../data/train.feather\")\n",
    "\n",
    "# Save features and categorical features\n",
    "features = [x for x in train.columns if x not in ['id','target']]\n",
    "lgbm_cat_features = [x for x in features if train[x].dtype.name.startswith(\"int\")]\n",
    "hist_cat_features = [train[x].dtype.name.startswith(\"int\") for x in features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75a50a1",
   "metadata": {},
   "source": [
    "# Model 1: LGBMClassifier\n",
    "\n",
    "The first model we test is the [LGBMClassifier](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html) from the LightGBM library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2bf9e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_lightgbm():\n",
    "    start = time.time()\n",
    "    scores = np.zeros(NUM_FOLDS)\n",
    "    print('')\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = 0)\n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['target'])):\n",
    "        \n",
    "        # train, valid split for cross-validation\n",
    "        X_train, y_train = train[features].iloc[train_idx], train['target'].iloc[train_idx]\n",
    "        X_valid, y_valid = train[features].iloc[valid_idx], train['target'].iloc[valid_idx]\n",
    "\n",
    "        # model with params\n",
    "        model = LGBMClassifier(\n",
    "            n_estimators = NUM_TREES,\n",
    "            random_state = RANDOM_SEED,\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set = [(X_valid, y_valid)],\n",
    "            eval_metric = 'auc',\n",
    "            early_stopping_rounds = EARLY_STOP,\n",
    "            categorical_feature = lgbm_cat_features,\n",
    "            verbose = False,\n",
    "        )\n",
    "\n",
    "        valid_preds = model.predict_proba(X_valid)[:,1]\n",
    "        \n",
    "        scores[fold] = roc_auc_score(y_valid, valid_preds)\n",
    "        print(f\"Fold {fold} (AUC):\", scores[fold])\n",
    "        \n",
    "    end = time.time()\n",
    "    return scores.mean(), round(end-start, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70584cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 (AUC): 0.8551987614596401\n",
      "Fold 1 (AUC): 0.8551955781455962\n",
      "Fold 2 (AUC): 0.8550151870874342\n",
      "\n",
      "Training Time: 463.0\n",
      "Holdout (AUC): 0.8551365088975569\n"
     ]
    }
   ],
   "source": [
    "lgbm_score, lgbm_time = score_lightgbm()\n",
    "\n",
    "print(\"\\nTraining Time:\", lgbm_time)\n",
    "print(\"Holdout (AUC):\", lgbm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23864751",
   "metadata": {},
   "source": [
    "# Model 2: HistGradientBoostingClassifier\n",
    "\n",
    "The second model we consider is the [HistGradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier) from scikit-learn, which itself is modeled after LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f2c836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_histgbm():\n",
    "    \n",
    "    start = time.time()\n",
    "    scores = np.zeros(NUM_FOLDS)\n",
    "    print('')\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = 0)\n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['target'])):\n",
    "        \n",
    "        # train, valid split for cross-validation\n",
    "        X_train, y_train = train[features].iloc[train_idx], train['target'].iloc[train_idx]\n",
    "        X_valid, y_valid = train[features].iloc[valid_idx], train['target'].iloc[valid_idx]\n",
    "\n",
    "        # model with params\n",
    "        model = HistGradientBoostingClassifier(\n",
    "            max_iter = NUM_TREES,\n",
    "            early_stopping = True,\n",
    "            n_iter_no_change = EARLY_STOP,\n",
    "            categorical_features = hist_cat_features,\n",
    "            validation_fraction = 0.1,\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        valid_preds = model.predict_proba(X_valid)[:,1]\n",
    "        \n",
    "        scores[fold] = roc_auc_score(y_valid, valid_preds)\n",
    "        print(f\"Fold {fold} (AUC):\", scores[fold])\n",
    "        \n",
    "    end = time.time()\n",
    "    return scores.mean(), round(end-start, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eae5de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 (AUC): 0.854598950948193\n",
      "Fold 1 (AUC): 0.8547508884946713\n",
      "Fold 2 (AUC): 0.8546138417438963\n",
      "\n",
      "Training Time: 659.82\n",
      "Holdout (AUC): 0.8546545603955868\n"
     ]
    }
   ],
   "source": [
    "hist_score, hist_time = score_histgbm()\n",
    "\n",
    "print(\"\\nTraining Time:\", hist_time)\n",
    "print(\"Holdout (AUC):\", hist_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
