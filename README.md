# README

The tabular playground series is a monthly playground-style competition involving synthetic tabular data generated by a CTGAN. I wanted to group all my work for this competition series into one repository to reduce some clutter.

## TPS 12 - December

The current (and maybe final) tabular playground series competition. My goal with this competition is to focus on feature engineering and selection since this competition includes non-anonymized features. To be continued...

## TPS 11 - November

[This month's competition](https://www.kaggle.com/c/tabular-playground-series-nov-2021/overview) is a binary classification problem involving a synthetic dataset with 100 numerical features and a binary target.

**Edit:** Unfortunately, the organizers released an [earlier version](https://www.kaggle.com/criskiev/november21) of the training data by accident. Some kagglers noticed that the re-released training data has about ~25% intentionally mislabeled targets, see the discussion [here](https://www.kaggle.com/c/tabular-playground-series-nov-2021/discussion/285503). So we should expect to top out at around 0.75 test AUC.

My main goals for this month's competition are as follows:

* Test models other than the typical gradient boosting libraries.
* Explore [cleanlab](https://github.com/cleanlab/cleanlab) library for dealing with noisy labels.
* Build a simple Keras model

## TPS 10 - October

This was my third official Kaggle competition. My main goal for this competition was to improve at hyperparameter searches. Other things I experimented with:

* Adversarial validation
* Optuna integrations (pruning callbacks for `XGBoost` and `LightGBM`)
* Tensorflow Decision Forests

**Note:** The Tensorflow Decision Forests library currently requires a linux environment so I decided to use Kaggle notebooks instead of my local computer. You can see my TFDF baseline [here](https://www.kaggle.com/rsizem2/tps-10-21-tensorflow-decision-forests-baseline) and some benchmarks I did with various training set sizes [here](https://www.kaggle.com/rsizem2/tps-10-21-tensorflow-decision-forests-benchmarks). I don't think this library is currently ready to be used seriously in Kaggle competitions mostly because it is very unoptimized in its current state (e.g. no GPU/TPU optimization).

## TPS 09 - September

This was my second official Kaggle competition. My main focus was on hyperparameter tuning using the optuna framework. My local machine does not have a GPU so I mostly used[LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html). For my work relating to this competition involving other models including [XGBoost](https://xgboost.readthedocs.io/en/latest/) and [Tensorflow Decision Forests](https://www.tensorflow.org/decision_forests) see my public Kaggle notebooks [here](https://www.kaggle.com/rsizem2/tps0921foldsfeather/code).

